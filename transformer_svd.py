import torch
import torch.nn as nn
import math

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=10):
        super(PositionalEncoding, self).__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)

    def forward(self, x, iter_idx):
        # x is the input tensor, iter_idx is the current iteration number
        # self.pe is (1, max_len, d_model)
        # We select the positional encoding for the current iteration and add it to x
        return x + self.pe[:, iter_idx, :].unsqueeze(1) # Add to sequence dimension

class SVDTransformer(nn.Module):
    def __init__(self, input_dim=64, embed_dim=64, rank=32, nhead=8, num_encoder_layers=6, 
                 dim_feedforward=256, dropout=0.1, max_iters=10, use_conv_encoder=False, use_vit_encoder=False):
        super(SVDTransformer, self).__init__()
        self.input_dim = input_dim
        self.rank = rank
        self.embed_dim = embed_dim
        self.max_iters = max_iters
        self.use_conv_encoder = use_conv_encoder
        self.use_vit_encoder = use_vit_encoder

        # 1. Input Encoder - ä¿æŒå…±äº«ï¼ˆå› ä¸ºè¾“å…¥Hæ˜¯å›ºå®šçš„ï¼‰
        if use_vit_encoder:
            # ViTç¼–ç å™¨é…ç½®ä¿æŒä¸å˜
            self.patch_size = 8
            self.num_patches = (input_dim // self.patch_size) ** 2
            self.patch_dim = self.patch_size * self.patch_size * 2
            
            self.patch_embedding = nn.Linear(self.patch_dim, embed_dim)
            self.patch_pos_embedding = nn.Parameter(torch.randn(1, self.num_patches, embed_dim))
            
            vit_layer = nn.TransformerEncoderLayer(
                d_model=embed_dim, 
                nhead=nhead//2,
                dim_feedforward=dim_feedforward//2, 
                dropout=dropout, 
                batch_first=True
            )
            self.vit_transformer = nn.TransformerEncoder(vit_layer, num_layers=2)
            self.patch_to_row = nn.Linear(embed_dim, embed_dim)
            
        elif use_conv_encoder:
            self.h_conv_encoder = nn.Sequential(
                nn.Conv1d(in_channels=input_dim*2, out_channels=embed_dim*2, kernel_size=3, padding=1),
                nn.ReLU(),
                nn.Conv1d(in_channels=embed_dim*2, out_channels=embed_dim, kernel_size=3, padding=1),
                nn.ReLU(),
            )
        else:
            self.h_encoder = nn.Linear(input_dim * 2, embed_dim)

        # 2. ä¸ºæ¯ä¸ªé˜¶æ®µåˆ›å»ºç‹¬ç«‹çš„SVDç»„ä»¶åµŒå…¥å±‚
        self.stage_u_embeds = nn.ModuleList([
            nn.Linear(input_dim * 2, embed_dim) for _ in range(max_iters)
        ])
        self.stage_v_embeds = nn.ModuleList([
            nn.Linear(input_dim * 2, embed_dim) for _ in range(max_iters)
        ])
        self.stage_s_embeds = nn.ModuleList([
            nn.Linear(rank, embed_dim) for _ in range(max_iters)
        ])

        # 3. ä¸ºæ¯ä¸ªé˜¶æ®µåˆ›å»ºç‹¬ç«‹çš„Transformerç¼–ç å™¨
        self.stage_transformers = nn.ModuleList()
        for i in range(max_iters):
            encoder_layer = nn.TransformerEncoderLayer(
                d_model=embed_dim, 
                nhead=nhead, 
                dim_feedforward=dim_feedforward, 
                dropout=dropout, 
                batch_first=True
            )
            transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)
            self.stage_transformers.append(transformer)

        # 4. ä¸ºæ¯ä¸ªé˜¶æ®µåˆ›å»ºç‹¬ç«‹çš„è§£ç å™¨
        self.stage_u_decoders = nn.ModuleList([
            nn.Linear(embed_dim, input_dim * 2) for _ in range(max_iters)
        ])
        self.stage_v_decoders = nn.ModuleList([
            nn.Linear(embed_dim, input_dim * 2) for _ in range(max_iters)
        ])
        self.stage_s_decoders = nn.ModuleList([
            nn.Linear(embed_dim, rank) for _ in range(max_iters)
        ])

        # 5. ä½ç½®ç¼–ç ï¼ˆä¿æŒå…±äº«ï¼‰
        self.pos_encoder = PositionalEncoding(embed_dim, max_len=max_iters)

        # 6. åˆå§‹å‚æ•°ï¼ˆä¿æŒå…±äº«ï¼‰
        self.initial_U = nn.Parameter(torch.randn(1, rank, input_dim, 2))
        self.initial_V = nn.Parameter(torch.randn(1, rank, input_dim, 2))
        self.initial_S = nn.Parameter(torch.randn(1, rank))

    def forward(self, x, iterations=4, return_all_stages=False):
        batch_size = x.shape[0]
    
        # ç¼–ç è¾“å…¥çŸ©é˜µHï¼ˆä½¿ç”¨å…±äº«ç¼–ç å™¨ï¼‰
        if self.use_vit_encoder:
            patches = self._extract_patches(x)
            patch_tokens = self.patch_embedding(patches)
            patch_tokens = patch_tokens + self.patch_pos_embedding
            vit_output = self.vit_transformer(patch_tokens)
            encoded_h = self.patch_to_row(vit_output)
        elif self.use_conv_encoder:
            h_reshaped = x.view(batch_size, self.input_dim, self.input_dim * 2)
            h_transposed = h_reshaped.transpose(1, 2)
            encoded_h_conv = self.h_conv_encoder(h_transposed)
            encoded_h = encoded_h_conv.transpose(1, 2)
        else:
            h_reshaped = x.view(batch_size, self.input_dim, self.input_dim * 2)
            encoded_h = self.h_encoder(h_reshaped)
    
        # åˆå§‹åŒ–SVDåˆ†é‡
        pred_U = self.initial_U.expand(batch_size, -1, -1, -1)
        pred_V = self.initial_V.expand(batch_size, -1, -1, -1)
        pred_S = self.initial_S.expand(batch_size, -1)
    
        if return_all_stages:
            all_stages = []
    
        # è¿­ä»£ç»†åŒ– - æ¯ä¸ªé˜¶æ®µä½¿ç”¨ç‹¬ç«‹æƒé‡
        for i in range(iterations):
            # ä½¿ç”¨ç¬¬ié˜¶æ®µçš„ç‹¬ç«‹åµŒå…¥å±‚
            u_flat = pred_U.flatten(start_dim=2)
            u_tokens = self.stage_u_embeds[i](u_flat)
    
            v_flat = pred_V.flatten(start_dim=2)
            v_tokens = self.stage_v_embeds[i](v_flat)
    
            s_tokens = self.stage_s_embeds[i](pred_S).unsqueeze(1)
    
            # æ‹¼æ¥tokens
            tokens = torch.cat([encoded_h, u_tokens, v_tokens, s_tokens], dim=1)
    
            # æ·»åŠ ä½ç½®ç¼–ç 
            tokens = self.pos_encoder(tokens, i)
    
            # ä½¿ç”¨ç¬¬ié˜¶æ®µçš„ç‹¬ç«‹Transformer
            output_tokens = self.stage_transformers[i](tokens)
    
            # åˆ†å‰²è¾“å‡ºtokens
            h_out_len, u_out_len, v_out_len = self.input_dim, self.rank, self.rank
            u_start, v_start = h_out_len, h_out_len + u_out_len
            
            u_out = output_tokens[:, u_start:v_start, :]
            v_out = output_tokens[:, v_start:v_start + v_out_len, :]
            s_out = output_tokens[:, -1, :]
    
            # ä½¿ç”¨ç¬¬ié˜¶æ®µçš„ç‹¬ç«‹è§£ç å™¨
            delta_U_flat = self.stage_u_decoders[i](u_out)
            delta_U = delta_U_flat.view(batch_size, self.rank, self.input_dim, 2)
    
            delta_V_flat = self.stage_v_decoders[i](v_out)
            delta_V = delta_V_flat.view(batch_size, self.rank, self.input_dim, 2)
    
            delta_S = self.stage_s_decoders[i](s_out)
    
            # æ›´æ–°SVDåˆ†é‡
            pred_U = pred_U + delta_U
            pred_V = pred_V + delta_V
            pred_S = pred_S + delta_S
            
            # æ·»åŠ åˆ—å½’ä¸€åŒ–ï¼ˆå¯é€‰ï¼‰
            pred_U = self.normalize_columns(pred_U)
            pred_V = self.normalize_columns(pred_V)
    
            if return_all_stages:
                all_stages.append((pred_U.clone(), pred_S.clone(), pred_V.clone()))
    
        if return_all_stages:
            return all_stages
        else:
            return pred_U, pred_S, pred_V
    def normalize_columns(self, mat):
        """
        å¯¹å¤æ•°çŸ©é˜µçš„åˆ—è¿›è¡Œå½’ä¸€åŒ–
        mat: (B, rank, input_dim, 2) - æœ€åä¸€ç»´æ˜¯ [å®éƒ¨, è™šéƒ¨]
        è¿”å›: å½’ä¸€åŒ–åçš„çŸ©é˜µ
        """
        real = mat[..., 0]  # (B, rank, input_dim)
        imag = mat[..., 1]  # (B, rank, input_dim)
        
        # è®¡ç®—å¤æ•°æ¨¡é•¿: sqrt(real^2 + imag^2)
        norm = torch.sqrt(real ** 2 + imag ** 2 + 1e-8)  # (B, rank, input_dim)
        
        # æ‰©å±•ç»´åº¦ä»¥åŒ¹é…åŸçŸ©é˜µ
        norm = norm.unsqueeze(-1).expand_as(mat)  # (B, rank, input_dim, 2)
        
        # å½’ä¸€åŒ–
        mat = mat / norm
        return mat

    def _extract_patches(self, x):
        """
        å°†è¾“å…¥çŸ©é˜µåˆ†å‰²æˆpatches
        x: (B, 64, 64, 2)
        è¿”å›: (B, num_patches, patch_dim) = (B, 64, 128)
        """
        batch_size = x.shape[0]
        
        # ä½¿ç”¨unfoldæå–patches
        # x: (B, 64, 64, 2) -> (B, 2, 64, 64)
        x_permuted = x.permute(0, 3, 1, 2)
        
        # æå–8x8çš„patches
        patches = x_permuted.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)
        # patches: (B, 2, 8, 8, 8, 8)
        
        # é‡æ–°æ•´ç†ç»´åº¦
        patches = patches.contiguous().view(batch_size, 2, 8, 8, -1)
        # patches: (B, 2, 8, 8, 64)
        
        patches = patches.permute(0, 4, 1, 2, 3)
        # patches: (B, 64, 2, 8, 8)
        
        # å±•å¹³æ¯ä¸ªpatch
        patches = patches.view(batch_size, self.num_patches, -1)
        # patches: (B, 64, 128)
        
        return patches
    

def test_progressive_training():
   
    """æµ‹è¯•æ¸è¿›å¼è®­ç»ƒçš„è¾“å‡ºç»´åº¦"""
    print("\n=== æ¸è¿›å¼è®­ç»ƒè¾“å‡ºæµ‹è¯• ===")
    
    batch_size = 2
    input_dim = 64
    rank = 32
    embed_dim = 128
    iterations = 4
    
    model = SVDTransformer(
        input_dim=input_dim,
        embed_dim=embed_dim,
        rank=rank,
        max_iters=10
    )
    
    test_input = torch.randn(batch_size, input_dim, input_dim, 2)
    print(f"è¾“å…¥ç»´åº¦: {test_input.shape}")
    
    # æµ‹è¯•è¿”å›æ‰€æœ‰é˜¶æ®µçš„è¾“å‡º
    with torch.no_grad():
        all_stages = model(test_input, iterations=iterations, return_all_stages=True)
    
    print(f"\næ€»å…± {len(all_stages)} ä¸ªè¿­ä»£é˜¶æ®µ:")
    for i, (U, S, V) in enumerate(all_stages):
        print(f"é˜¶æ®µ {i+1}:")
        print(f"  U: {U.shape} (å®Œæ•´32ä¸ªåˆ†é‡)")
        print(f"  S: {S.shape} (å®Œæ•´32ä¸ªåˆ†é‡)")
        print(f"  V: {V.shape} (å®Œæ•´32ä¸ªåˆ†é‡)")
        
        # éªŒè¯ç»´åº¦
        assert U.shape == (batch_size, rank, input_dim, 2)
        assert S.shape == (batch_size, rank)
        assert V.shape == (batch_size, rank, input_dim, 2)
    
    print("\nâœ… æ‰€æœ‰é˜¶æ®µè¾“å‡ºç»´åº¦æ­£ç¡®ï¼")
    print("ğŸ’¡ æ¯ä¸ªé˜¶æ®µéƒ½è¾“å‡ºå®Œæ•´çš„32ä¸ªSVDåˆ†é‡ï¼Œå…·ä½“ç›‘ç£ç­–ç•¥åœ¨è®­ç»ƒè„šæœ¬ä¸­å®šä¹‰")
    
    return all_stages

# åœ¨ä¸»å‡½æ•°ä¸­æ·»åŠ æµ‹è¯•
if __name__ == "__main__":
    print("=== SVDTransformer ç»´åº¦æµ‹è¯• ===")
    
    # æµ‹è¯•å‚æ•°
    batch_size = 4
    input_dim = 64
    rank = 32
    embed_dim = 128
    iterations = 4
    max_test_iterations = 10  # è®¾ç½®æ›´å¤§çš„æœ€å¤§è¿­ä»£æ•°
    
    # åˆå§‹åŒ–æ¨¡å‹ - ä½¿ç”¨æ›´å¤§çš„max_iters
    model = SVDTransformer(
        input_dim=input_dim,
        embed_dim=embed_dim,
        rank=rank,
        nhead=8,
        num_encoder_layers=6,
        dim_feedforward=256,
        dropout=0.1,
        max_iters=max_test_iterations  # æ”¹ä¸º10ï¼Œæ”¯æŒæ›´å¤šè¿­ä»£æµ‹è¯•
    )
    
    # è®¡ç®—æ¨¡å‹å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    print(f"ğŸ“Š æ¨¡å‹æ€»å‚æ•°é‡: {total_params:,}")
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    test_input = torch.randn(batch_size, input_dim, input_dim, 2)
    print(f"\nğŸ“¥ è¾“å…¥ç»´åº¦: {test_input.shape}")
    print(f"   - æ‰¹é‡å¤§å°: {batch_size}")
    print(f"   - çŸ©é˜µå°ºå¯¸: {input_dim}x{input_dim}")
    print(f"   - å¤æ•°è¡¨ç¤º: 2 (å®éƒ¨+è™šéƒ¨)")
    
    try:
        print("\nğŸ”„ å¼€å§‹å‰å‘ä¼ æ’­...")
        
        # å‰å‘ä¼ æ’­
        with torch.no_grad():
            U, S, V = model(test_input, iterations=iterations)
        
        print("âœ… å‰å‘ä¼ æ’­æˆåŠŸï¼")
        
        # æ£€æŸ¥è¾“å‡ºç»´åº¦
        print(f"\nğŸ“¤ è¾“å‡ºç»´åº¦æ£€æŸ¥:")
        print(f"   U çŸ©é˜µ: {U.shape} (æœŸæœ›: ({batch_size}, {rank}, {input_dim}, 2))")
        print(f"   S å‘é‡: {S.shape} (æœŸæœ›: ({batch_size}, {rank}))")
        print(f"   V çŸ©é˜µ: {V.shape} (æœŸæœ›: ({batch_size}, {rank}, {input_dim}, 2))")
        
        # éªŒè¯ç»´åº¦æ­£ç¡®æ€§
        expected_U = (batch_size, rank, input_dim, 2)
        expected_S = (batch_size, rank)
        expected_V = (batch_size, rank, input_dim, 2)
        
        assert U.shape == expected_U, f"Uç»´åº¦é”™è¯¯: å¾—åˆ°{U.shape}, æœŸæœ›{expected_U}"
        assert S.shape == expected_S, f"Sç»´åº¦é”™è¯¯: å¾—åˆ°{S.shape}, æœŸæœ›{expected_S}"
        assert V.shape == expected_V, f"Vç»´åº¦é”™è¯¯: å¾—åˆ°{V.shape}, æœŸæœ›{expected_V}"
        
        print("\nâœ… æ‰€æœ‰ç»´åº¦æ£€æŸ¥é€šè¿‡ï¼")
        
        # æ•°å€¼èŒƒå›´æ£€æŸ¥
        print(f"\nğŸ“ˆ æ•°å€¼èŒƒå›´æ£€æŸ¥:")
        print(f"   U èŒƒå›´: [{U.min():.4f}, {U.max():.4f}]")
        print(f"   S èŒƒå›´: [{S.min():.4f}, {S.max():.4f}]")
        print(f"   V èŒƒå›´: [{V.min():.4f}, {V.max():.4f}]")
        
        # æµ‹è¯•é‡æ„è¯¯å·®ï¼ˆç®€å•éªŒè¯ï¼‰
        print(f"\nğŸ” é‡æ„éªŒè¯:")
        
        # å°†å¤æ•°è¡¨ç¤ºè½¬æ¢ä¸ºå¤æ•°å¼ é‡è¿›è¡ŒçŸ©é˜µä¹˜æ³•
        def complex_matmul(U, S, V):
            # U: (B, rank, 64, 2), S: (B, rank), V: (B, rank, 64, 2)
            # è½¬æ¢ä¸ºå¤æ•°
            U_complex = torch.complex(U[..., 0], U[..., 1])  # (B, rank, 64)
            V_complex = torch.complex(V[..., 0], V[..., 1])  # (B, rank, 64)
            
            # é‡æ„: U @ diag(S) @ V^H
            # U: (B, rank, 64), S: (B, rank), V: (B, rank, 64)
            S_expanded = S.unsqueeze(-1)  # (B, rank, 1)
            US = U_complex * S_expanded  # (B, rank, 64)
            
            # V^H: (B, 64, rank)
            V_H = V_complex.conj().transpose(-2, -1)
            
            # é‡æ„çŸ©é˜µ: (B, 64, 64)
            reconstructed = torch.bmm(US.transpose(-2, -1), V_H.transpose(-2, -1))
            
            return reconstructed
        
        try:
            # åŸå§‹è¾“å…¥è½¬ä¸ºå¤æ•°
            input_complex = torch.complex(test_input[..., 0], test_input[..., 1])
            
            # é‡æ„
            reconstructed = complex_matmul(U, S, V)
            
            # è®¡ç®—é‡æ„è¯¯å·®
            reconstruction_error = torch.norm(input_complex - reconstructed, dim=(-2, -1)).mean()
            print(f"   å¹³å‡é‡æ„è¯¯å·®: {reconstruction_error.item():.6f}")
            
        except Exception as e:
            print(f"   é‡æ„éªŒè¯è·³è¿‡ (å¤æ•°è¿ç®—é—®é¢˜): {e}")
        
        # æµ‹è¯•ä¸åŒæ‰¹é‡å¤§å°
        print(f"\nğŸ”„ æµ‹è¯•ä¸åŒæ‰¹é‡å¤§å°:")
        for test_batch in [1, 2, 8]:
            test_input_batch = torch.randn(test_batch, input_dim, input_dim, 2)
            with torch.no_grad():
                U_batch, S_batch, V_batch = model(test_input_batch, iterations=2)
            print(f"   æ‰¹é‡å¤§å° {test_batch}: U{U_batch.shape}, S{S_batch.shape}, V{V_batch.shape} âœ…")
        
        # æµ‹è¯•ä¸åŒè¿­ä»£æ¬¡æ•° - ä¿®å¤ï¼šç¡®ä¿ä¸è¶…è¿‡max_iters
        print(f"\nğŸ”„ æµ‹è¯•ä¸åŒè¿­ä»£æ¬¡æ•°:")
        for test_iter in [1, 2, 6, 8]:  # ç°åœ¨å¯ä»¥å®‰å…¨æµ‹è¯•åˆ°8æ¬¡è¿­ä»£
            with torch.no_grad():
                U_iter, S_iter, V_iter = model(test_input, iterations=test_iter)
            print(f"   è¿­ä»£æ¬¡æ•° {test_iter}: è¾“å‡ºç»´åº¦ä¸€è‡´ âœ…")
        
        print(f"\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ¨¡å‹ç»´åº¦è®¾è®¡æ­£ç¡®ã€‚")
        
        # æ¨¡å‹ä¿¡æ¯æ€»ç»“
        print(f"\nğŸ“‹ æ¨¡å‹æ€»ç»“:")
        print(f"   - è¾“å…¥ç»´åº¦: (B, {input_dim}, {input_dim}, 2)")
        print(f"   - SVDç§©: {rank}")
        print(f"   - åµŒå…¥ç»´åº¦: {embed_dim}")
        print(f"   - Transformerå±‚æ•°: 6")
        print(f"   - æ³¨æ„åŠ›å¤´æ•°: 8")
        print(f"   - æœ€å¤§è¿­ä»£æ¬¡æ•°: {max_test_iterations}")
        print(f"   - æ€»å‚æ•°é‡: {total_params:,}")
        print(f"   - æ¨¡å‹å¤§å°: ~{total_params * 4 / 1024 / 1024:.1f} MB")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        
    print("\n=== æµ‹è¯•å®Œæˆ ===")
    
    # æ·»åŠ æ¸è¿›å¼è®­ç»ƒæµ‹è¯•
    test_progressive_training()


